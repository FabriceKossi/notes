= Devoxx France 2023
:imagesdir: ./images
:toc:

== Mercredi

=== Une Architecture GitOps from scratch : Gitlab, Ansible, Terraform, Kubernetes et AWS

.Speakers : Loïc Ortola et Aurélien Moreau (@Takima)

Infra complete en GitOps:
C'est quoi une infra complete? archi 3 tier

Stack appli:front en angular, Api spring boot
Database pgsql 
Kubernetes 
Prometheus pour la surveillance 
Terraform Ansible. 
(photo) 

Point de départ :compte AWS, un domaine acheté chez Gandi

Crash course
==== infra as code
1. Provisioning d'infra (hardware) . Il me faut des serveurs pour pouvoir host mon infra (serveurs, réseau etc... Provisioning d'knfra->cloud provider permettende monter des serveur plus facilement 
2.Configuration management installatiln du socle technique qui va nous permettre de faire tourner mon appli. management
3orchestration d'application. Faire tourner les appli, gérer le cycle de vie de l'app, gestio'
L'infra as code c'est faire tout ça ensemble 

IaC c'est faire sous forme de code 

==== Gitops
L'iaC comme d'écrire précédemment te représente ton truc à un temps t
Mais faut le versionner.
Gitops c'est d'avoir une source de vérité Git, dans lequel je mets mon code d'infra que je vais pouvoir synchroniser à un instant t par rapport à ce qui est déployé

==== De docker à Kubernetes
Ce qu'on produit en tant que dev c'est un artefact, le runtime c'est ce qui va permettre de faire tourner l'art
Le problème est que souvent c'est les équipe ops qui mettent en place le runtime alors que c'est les devs qui l'utilisent.
Donc dans l'idée un truc cool serait que les dev gèrent le runtime et c'est ça que permets docker. Le dev gère son environnement technique grâce à des container. 
L'artefact a changé. C'est maintenant une image docker comprenant le code source+ le containers ce qui permets lidem potence.

Quelques limitations tout de même en terme de gestion.
On a besoin d'un chef d'orchestre pour gérer tout ça que docker compose permet pas de gérer.
Un docker compose sur un seul serveur ça passe. Plusisuer docker compose sur 1 serveur ca devient galère. Plusieurs sur plusieurs serveur ça devient impossible docker compose sert pas à ça.
L'orchestrator par défaut pour gérer tout ça maintenant c'est Kubernetes

==== Kubernetes
Ça rend facile le management d'application (photo) 

Point archi:d'un côté les worker nodes qui font tourner les container de l'autre les master nodes qui font la gestion
Kube api via cette api on gère tout ! De Kubernetes.
**Dans Kubernetes TOUT est resources.**
On va d'écrire ces resources en yaml indiquant la resources vers cet état.
Et Kubernetes va faire en sorte de faire converger ces resources vers l'état qui nous intéresse


Resources de base de Kubernetes : le pod en gros un container
Replica set: Une consigne permettant de répliquer x fois une ressources
Le deployment : le container que je veux lancer, dans quelle version et quel état
Le deployment est donc la super ressource pour gérer les images successive de l'application. Il embarqué nos éléments précédents (Photos)

Configmap +secrets: injecter variables d'env +fichiers à nos pods
Differences config map en clair et secrets en chiffrés

Service :va nous permettre de Maintenant in veut publier l'applicatif. Le service permets de jouer les load balancer en interne.
Si on veut publier sur internet on va plutôt utiliser **l'ingress** c'est un reverse proxy.
Contrairement aux autre ressources l'ingress marche pas comme par défaut.
Le **namespace** espace de nommage regroupant toutes les ressources lié à un projet.

=== Episode 1 déployer infra
==== EKS
Eks c'est le Kubernetes managé propose par AWS d'amazone
Particularité d'eks
AZ=avaibility zone= centre réseau entier pour aws
==== terraform

Solution d'automation de l'infra et de providing des resources cloud.

==== GitlabCicd
Solution de choix pour le déploiement

==== Architecture:
Un dossier sur gitlab. Repo Terraform qu'on ça déployer chez Aws

Code Terraform des fichiers en. Tf.
On va d'écrire sous forme de code notre infra
On va indiquer notre provider, où il est déployé.
Les ressources, déclaré en snake case et indiquer sur quel provider c'est envoyé.
Declaratiin de Var (photo)
On peut fournir un fichier contenant toutes les variablss
Ou on peu déclarer des variables d'env
Ou on peut tout déclarer en lignes de commandes

Comment connaître l'ip du PROVIDER? => Variables output.
Le fichier. Tfstate est un fichier de sortie donnant l'ensemble des informations de notre infra déployée.
Mais fichier três três verbeux Donc Il est plus simple de définir aussi des variables d e sortie pour flag les infos qui nous interesse

Les modules permettent de recouper tout ça.

=== cycle de vie.
Terraform init pour initialiser tout ça.
Le plan va resynchro notre app avec le Tfstate.
L'apply fait le déploiement et défait le tfstate
Le destroy permet de tout effacer

=== Ansible :plate-forme pour configurer et manager des plate-forme
Controle node pour manager Ansible et ses modules. 
Inventaire permet de lister les machines notamment. Mais on va a plus l'utiliser pour lister où doivent être deployé les éléments 

Un playbook est une procédure d'install technique c'est ce que l'on d'exploit. 
Et on le déploie sur l'inventaire 

Ansible utilise aussi des modules et on peut remarque que comme Terraform on peut déclarer l'état attendu. 

Comment se connecter à K8s ? 
Le kube-config
Terraform ça filer un tf.state,que Ansible va aller chopper pour faire sa config

Artifacthub, site permettant de récupérer vite des packages.
Prendre le soin de transformer les commandes en module pour faire du déclaratif plutôt que de l'impératif 


Terraform comme Ansible vont utiliser le Tf. State

Terraform est la pour gérer des ressources.
Dans Ansible par contre on a des notions de tasks ! 
Donc si je veux par exemple demander d'attendre 5mn qu'un service soit up, Terraform est pas. Vraiment fait pour ça. 

Ansible n'a par contre pas de notion de tf state donc pas de manière de vraiment aller stocker des états et sauvegarder nos vars à réutiliser 

Donc des besoins différents remplis par chaque outils. 



===PGSL

Notre base de donnée on la veut persistence par contre. Contrairement aux pods que l'on kill et recree

Bdd production ready: (photo) 
On pourrait utiliser le rdd Amazon qui filé tout ça production ready mais ca coûte une blinde et est très lié à Amazon 
On va donc se le créer nous meme

Dans K8S on a ce qu'on appelle un **Operator** c'est une ressource permettant de créer de nouvelles ressources. 
Puisque la nouvelle ressource est custom il nous faut un Controller qui est le cerveau qui interprétera la nouvelles ressource. 
On va donc creee

BucketS3 permet de stocker des infos, on va donc y persister les infos de notre bdd

A retenir de K8S  (photos) 

=== episode 4 Mes environnement

On veut passer de 1 à 'environnements. Va falloir modifier pas mal de trucs. 

Côte Terraform. On va ajouter un front end et un backend en créant des clusters

Côté Ansible on va juste mettre à jour l' inventory 
Côté K8S modif des yaml-> on va utiliser un moteur de templating. 
On va utiliser helm qu'on utilisait plus tôt juste comme manager de ressources mais on peut aussi l'utiliser pour gérer le templating via le 
Vqriables

Pipeline gitlab.
On va créer un cluster tesch'ique qui sera transient aux autres enviromment les elmements du cluster technique seront communs aux autres
Rancher outils supplémentaire d'administration 
Creatikn d'un projet en admin





Monitoring Centralisé par l'outils

Argocd, chef de chantier. Va comparer l'état des spes d'app déployé avec l'attendu et remonte des alertes si desynchro
Faire cette conf en manuel ok c'est faisable mais si j'ai 90 appli on va pas faire ça. Fort heureusement  dans Kubernetes tous est ressources ! 
Les éléments proposé par Argo peuvent eux aussi être déclaré en ressources et scriptés


===Questions

Le code est très lié au cloud provider Donc so on change de cloud provider faut recoder, les apis à appeler doivent être mis à jour aussi.

Comment gerer les secret dans k8s. Deux manières
Le silksecret :chiffrer les secrets avec une clé posée dans un repo git mais difficile de faire de la rotation 
GoSecret projet gérer par la communauté marche via un secret store
